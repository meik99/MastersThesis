\chapter{Introduction}
\label{ch:introduction}

In recent years, a set of practices have emerged with the goal of supporting software development.
Expanding on agile practices, these have the same goal albeit with other means.
This set of practices is commonly referred to as ``DevOps'' practices.
A short summary, which is explored shortly, is:

``DevOps is IT for software developers.''\footnote{A quote once overheard in passing.}

Ebert et.al.\ explain the field as an integration of two worlds~\cite{DevOps}: \\
Automating development and operations; \\
Automating integration and delivery; \\
Automating testing and deploying; \\
Automating logging and monitoring;

The first need that arises to achieve this level of automation is the need for tools.
Again, Ebert et.al., as well as Leite et.al, identify this necessity~\cite{DevOps, ASurveyofDevOpsConceptsandChallenges}.
First and foremost, build tools are required to automate integrating software systems, most of which also include some form of dependency management to automatically link required libraries to the resulting system.

Then, testing tools are required to automate testing newly integrated code and therefore streamline automatic integration.
Since this process is also called Continuous Integration (CI), these tools are known as continuous integration tools~\cite{DevOps}.
Closely related are the Continuous Delivery (CD) tools.
These satisfy the need of automatically delivering or deploying newly integrated code.
Usually, a tool that does continuous integration also does continuous delivery because of that fact.
They are thus often called CI/CD tools for short.

\pagebreak

Taking back a step, agile methods, as discussed in the beginning, have the goal of increasing the velocity of software development~\cite{ADecadeOfAgileMethodologies}.
One of the principles these methods follow to achieve this goal is collective code ownership~\cite{CommonAgilePracticesInSoftwareProcesses, ManagingCodeOwnership}.
Until fairly recently, pipelines to automatically integrate and deliver code were done using a graphical user interface (GUI)~\cite{JenkinsClassicUi}.
As this practice slowly faded out, infrastructure as code emerged~\cite{ASystematicMappingStudyOfInfrastructureAsCodeResearch}.
Since the infrastructure is now represented as code, it too can be collectively owned, enabling more DevOps practices and further increasing development velocity.

Summarizing briefly, it is now established that automated pipelines exist and are used.
Their configuration is shared code amongst the members of a software team or other stakeholders.
They are used to automatically integrate code into a system and then deliver this system to customers.
These steps give a nice overview of the overall process, yet some details are still missing.

First, it must be discussed how to verify that the newly and automatically integrated code works well and as intended.
Secondly, it must be decided where to deliver the system and how to do this automatically and correctly.
After all, the system cannot just be put somewhere on the internet with the hopes that the customer will eventually figure out how to use it.
Changelogs, version bumps, installation instructions and more must be carefully considered.
As this thesis shows, automating the maintenance of this secondary information must be done in a thoughtful and precise manner.

For many basic systems, the story ends with basic tests.
They are unit tested, integration tests cover multiple units and system tests make sure features as a whole work correctly and as intended by simply executing a program.
In a system's lifetime, tests are designed and implemented.
When the system grows so do its tests.
Everytime new code is written, tests are added, previous ones are run and the code is integrated.

The problem is solved.
Well, not entirely.
One small type of complex systems still holds out against the tests.
And life is not easy for the testers, who garrison the test suits of components, architecture, design and requirements~\cite{AsterixBeginningSentence}.

\pagebreak

These systems are those that run in clusters, a collection of servers, varying in size, connected together fulfilling one or many business cases.
They exist and run in sometimes huge environments.
Lower-level testing, such as unit testing, is not much different or not different at all.
However, higher-level testing is more complex and resource intensive than testing on lower levels.
For example, to test such a system on the level of system testing, a cluster is needed for the system to run in.
This in turn means setting up servers, connecting them to a cluster and installing management software, even before the first test can be run.
After a test ran, this environment then needs to be destroyed and recreated, so that the next test can be run uninfluenced by the one that came before.

With great complexity comes a great learning curve.
Not all developers in a team have the domain knowledge to create and maintain a pipeline configuration for such a setup.
Therefore, one of the objectives of this thesis is to make creating pipeline configurations easier and more accessible by abstracting complicated domain knowledge to a more general interface.

The second problem after testing such highly complex systems is the delivery and distribution.
Usually, programs are delivered in the form of binaries, distributed often by a website or package management system.
As is shown later, the distribution of cluster systems is highly diverse with many so-called ``marketplaces'' from which that software can be deployed from.
Targeting many of these systems multiplies the problem touched before, needing more change-logs, setting versions correctly and, additionally, adhering to proprietary configurations for specific marketplaces.
The second objective is therefore, to integrate these multiple systems into one, automating as much as possible, while using a common base for the data and metadata that is needed.

In order to tackle the first problem, a system is developed to help abstract the complexity of maintaining higher level tests.
This system's purpose is to use a template of a configuration file and then generate a concrete configuration for a pipeline.
Going forward, this system is called the Dynamic Testing Framework (DTF).

\pagebreak

This method has a few advantages to manually maintaining configurations.
First, if a pipeline configuration contains a fault, a fix in the template fixes all concrete configurations derived from it.
Secondly, only one configuration must be maintained, avoiding a problem analogous to one known as ``shotgun-surgery''\footnote{\url{https://refactoring.guru/smells/shotgun-surgery}} in software engineering, were one small changes must be made in many different files.
Thirdly, once the template is written, the domain knowledge is sufficiently abstracted for other team members.
Using the system to generate a concrete configuration does not necessitate knowledge about the domain of pipeline configurations.

To address the second problem, another pipeline is created for a specific system.
This pipeline contains steps to gather information, build necessary files and then deploy those files to different platforms.
The platforms to be deployed are specific marketplaces for this system and will be discussed in Section~\ref{sec:release-pipeline}

The goal is to find and use data, that is common to each platform and prepare it as much as possible before distributing it.
Each platform may have special steps that need to be taken into account.
Combining these steps in small scripts may increase the maintainability of the whole pipeline.
Since the main purpose of this pipeline is to distribute a system, it is simply called ``Release pipeline''.

In order to solve the problems stated, this thesis first introduces the area of DevOps in which these problems fall into.
The background in Chapter~\ref{ch:background} also covers testing, virtualization, Kubernetes and similarity-based variability testing, since they are important concepts used in this thesis.
Architectural design decisions of the implementation are discussed in Chapter~\ref{ch:architecture} and the context is discussed in Chapter~\ref{ch:context}.

Chapter~\ref{ch:implementation} shows how the problems described have been solved.
One system, called the Dynamic Testing Framework (DTF), is used to generate a pipeline for tests.
The second system, another pipeline simply called a release pipeline, is used to generate releases.

\pagebreak

Both implementations are then evaluated in Chapter~\ref{ch:evaluation}.
The DTF is evaluated using a survey to establish if it is easier to use than the previously established process.
The release pipeline is evaluated by analysing the time needed for a release based on recorded time-tracking.
It shows that the DTF only abstracts complexity behind a different set of complexity.
However, the implementation of the release pipeline shows significant improvement in overall productivity.
Chapter~\ref{ch:conclusions-and-outlook} summarizes and concludes this work and discusses the need for more standards in the field of cloud computing.

